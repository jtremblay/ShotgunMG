process {
    // executor can be either 'local' or 'slurm' 
    executor = "slurm"
    clusterOptions = "--account=rrg-jtrembla --export=ALL"
}

params {
    clusterOptions = "--account=rrg-jtrembla --export=ALL"

    DEFAULT {
        /* 
            IMPORTANT PARAMETERS - will determine the workflow configuration.
        */
        cluster_time = 6.h
        cluster_cpus = 1
        cluster_memory = 12.GB
        // reads_config can be either 'paired' or 'single'
        reads_config = "paired"
        // Contaminants (sequencing adapters, phix, etc.) to remove. Will be used in the bbduk process.
        contaminants = "$INSTALL_HOME/databases/contaminants/Illumina.artifacts_phix.fa"
        // ref_genome_to_subtract is to be specified if you want to remove potential host DNA that
        // you do not wish to analyze. For instance, if you analyze metagenomic DNA from human feces samples and that 
        // the scope of your study is bacterial DNA only, you should subtract all potential human DNA.
        // you should include then include ref_genome_to_subtract="/path/to/genome/bbmap_indexed".
        // If you do not which to remove any potential host DNA, specify ref_genome_to_subtract = 'null'
        ref_genome_to_subtract = "$INSTALL_HOME/databases/genomes/homo_sapiens/bbmap_ref"
        // assembler software to use to co-assembled qced reads. Can be either 'megahit' or 'metaspades'
        assembler = "megahit"
        // binner (MAGs generator to use). either 'metabat2' or 'maxbin2'
        binner = "metabat2"
        // mapper. Software to use to map reads against co-assembly (i.e. to estimate abundance of each contig and gene of each sample.
        // can be mapper = 'bwa' or 'bbmap'. bwa will actually use bwa-mem.
        mapper = "bwa"
        // Other parameters that should usually stay the same from one project to another.
        raw_reads_dir = "./raw_reads"
        current_dir = "./"
        job_output_dir = "./jobs_output"
        // DO NOT FORGET TO SPECIFY A VALID TMP DIR.
        tmpdir = "/scratch/username/tmp/"
        raw_reads = "$projectDir/raw_reads/*_R{1,2}.fastq.gz"
        outdir = "$projectDir/output/"
        mapping_file = "$projectDir/mapping_file.tsv"
    }
    
    /*
        modules
    */
    modules {
        java = "java/17.0.2"
        trimmomatic = "nrc/trimmomatic/0.39"
        bbmap = "nrc/bbmap/39.00"
        perl = "nrc/perl/5.26.0"
        python = "nrc/python/3.9.0"
        R = "nrc/R/4.2.1"
        tools = "nrc/nrc_tools/1.3.2"
        blast = "nrc/blast/2.10.1+"
        bwa = "nrc/bwa/0.7.17"
        samtools = "nrc/samtools/1.9"
        bedtools = "nrc/bedtools/2.23.0"
        exonerate = "nrc/exonerate/2.2.0"
        hmmer = "nrc/hmmer/3.3.2"
        hdf5 = "nrc/hdf5/1.8.13"
        pigz = "nrc/pigz/2.3.4"
        metabat2 = "nrc/metabat/2.12.1"
        prodigal = "nrc/prodigal/2.6.3"
        checkm = "nrc/checkm/1.1.3"
        megahit = "nrc/megahit/1.2.9"
        pplacer = "nrc/pplacer/1.1"
        //diamond = "nrc/diamond/2.0.15"
        diamond = "nrc/diamond/2.0.8"
        CAT = "nrc/CAT/5.2.3"
        maxbin2 = "nrc/maxbin/2.2.7"
        spades = "nrc/SPAdes/3.15.0"
        quast = "nrc/quast/5.2.0-python3.9"
        trnascanse = "nrc/trnascan-se/2.0.7"
        barrnap = "nrc/barrnap/0.9"
        bedtoolsforbarrnap = "nrc/bedtools/2.29.2"
        infernal = "nrc/infernal/1.1.3"
        rtk = "nrc/rtk/0.93.2"
        fraggenescan = "nrc/fraggenescan/1.31"
        anvio = "nrc/anvio/7.0"
        kofamscan = "nrc/kofamscan/1.3.0"
        parallel = "nrc/parallel/20220322"
        bhtsne = "nrc/bhtsne/1.0"
        //python2 = "nrc/python/2.7.18"
        //bowtie2 = "nrc/bowtie2/2.3.2"
        //rdp_classifier = "nrc/rdp_classifier/2.5"
    }

    /*
        Customized parameters for individual processes
    */
    trimmomatic {    
        threads = 6
        trailing_min_quality = 30
        average_quality = 30
        quality_offset = 33
        min_length = 45
        sliding_window1 = 4
        sliding_window2 = 15
        // Have a look at the fastqc profiles to make sure you are cropping enough bases at the beginning of the reads.
        headcrop = 12
        // crop is optional. If for some reason you are still stuck with overrepresented kmers at the end, use crop=<int> to force them off.
        // just comment the parameter if no tail crop is needed.
        crop = 135
        adapter_fasta = "$INSTALL_HOME/databases/contaminants/adapters-nextera-xt.fa"
        illumina_clip_settings = ":2:10:10"
        cluster_time = 6.h
        cluster_cpus = 6
        cluster_memory = 32.GB
    }
    
    make_index {
        //TODO if large dataset, increase walltime
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 64.GB
    }
    
    gff_to_bed {
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 12.GB
    }

    megahit {
        // Megahit will use 0.9 pc of the node's memory by default.
        min_contig_length = 1000
        num_threads = 6
        kmin = 31
        kmax = 131
        kstep = 10
        //memory:fraction of the machine's total RAM to  use for SdBG construction. Aim for 90% of total memory available. Will multiply cluster_pmem value by fraction and convert to bytes.
        memory = 0.9
        //requested_memory in MB. Same value as cluster_pmem arg.
        //requested_memory=2900000
        requested_memory = 16000
        cluster_time = 3.h
        cluster_cpus = 6
        cluster_memory = 16.GB
    }

    spades {
        num_threads = 16
        min_contig_length = 1000
        kmers = [27,37,47,57,67,77,87,97,107,117,127]
        //Spades will use 250 Gb RAM by default. Plan around 90% or total node memory. Memory in GB
        //memory=3000
        //cluster_memory = 3095000.MB
        memory = 500
        cluster_time = 96.h
        cluster_cpus = 32
        cluster_memory = 256.GB
    }
    
    merge_pairs {
        cluster_time = 12.h
        cluster_cpus = 12
    }

    prodigal {
        cluster_time = 24.h
        cluster_cpus = 1
        cluster_memory = 32000.MB
    }

    bbmap {
        ram = "64g"
        min_id = 0.76
        num_threads = 4
        cluster_time = 12.h
        cluster_cpus = 4
        cluster_memory = 32000.MB
    }

    bbmap_sub {
        ram = "64g"
        min_id = 0.90
        num_threads = 4
        cluster_time = 12.h
        cluster_cpus = 4
        cluster_memory = 24000.MB
    }

    bwa {
        max_records_in_ram = 3750000
        cluster_time = 12.h
        num_threads = 12
        cluster_cpus = 12
        cluster_memory = 64000.MB
    }

    bbduk {
        k = 21
        s = 1
        c = 1
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 4000.MB
    }

    bedtools {
        //Do not use bedtools higher than v2.23 (v2.24 and above have RAM leakage issues).
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 32000.MB
    }

    CAT {
        r = 1
        f = 0.5
        database_folder = "$INSTALL_HOME/databases/CAT/CAT_prepare_20210107/2021-01-07_CAT_database"
        taxonomy_folder = "$INSTALL_HOME/databases/CAT/CAT_prepare_20210107/2021-01-07_taxonomy"
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 24000.MB
    }

    add_taxonomy {
        cutoff = 0.50
        tax_level = "best"
        cluster_time = 20.m
        cluster_cpus = 1
    }

    split_feature_table {
        cluster_time = 20.m
        cluster_cpus = 1
    }

    normalization {
        normalizationThreshold = 250
        multiplier = 100000000
        cluster_time = 10.h
        cluster_cpus = 1
        cluster_memory = 64000.MB
    }

    alpha_diversity {
        mode = "swap"
        perm = 10
        number_of_points = 30
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 32000.MB
    }

    summarize_taxonomy {
        taxonomyDepth = 8
        num_threads = 1
        cluster_time = 2.h
        cluster_cpus = 1
        cluster_memory = 12.GB
    }
   
    exonerate {
        //Targeted chunk file size is in MB
        targeted_chunk_file_size_contigs = 2
        targeted_chunk_file_size_genes = 2
        cluster_time = 12.h
        cluster_memory = 32000.MB
        cluster_cpus = 1
    }

    kegg {
        db = "$INSTALL_HOME/databases/kegg/2020-03-23/genes/fasta/genes.pep"
        genetoko = "$INSTALL_HOME/databases/kegg/2020-03-23/genes/links/genes_ko.list"
        ko = "$INSTALL_HOME/databases/kegg/2020-03-23/genes/ko/ko"
        genes_desc = "$INSTALL_HOME/databases/kegg/2020-03-23/genes/fasta/genes.tsv"
        KO_profiles = "$INSTALL_HOME/databases/kofam/2022-04-05/kofam.hmm"
    }
    
    pfam {
        db = "$INSTALL_HOME/databases/pfam/Pfam-A.hmm"
    }

    cog {
        db = "$INSTALL_HOME/databases/cog/2021-02-22/Cog"
    }

    kog {
        db = "$INSTALL_HOME/databases/kog/2021-02-22/Kog"
    }

    diamond_blastp {
        //db_nr = "$INSTALL_HOME/databases/CAT/CAT_prepare_20210107/2021-01-07_CAT_database/2021-01-07.nr.dmnd"
        db_nr = "$INSTALL_HOME/databases/CAT/CAT_prepare_20210107/2021-01-07_CAT_database_test/2021-01-07_test.nr.dmnd"
        evalue = 1e-02
        outfmt = 6
        //num_threads = 8
        //cluster_time = 24.h
        //cluster_cpus = 8
        //num_threads = 8
        cluster_time = 1.h
        cluster_cpus = 2
        cluster_memory = 8000.MB
    }

    rpsblast {
        evalue = 1e-10
        num_threads = 2
        outfmt = "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore stitle"
        cluster_time = 9.h
        cluster_cpus = 2
        cluster_memory = 12000.MB
    }

    hmmsearch {
        evalue = 1e-10
        num_threads = 6
        cluster_time = 12.h
        cluster_cpus = 6
        cluster_memory = 24000.MB
    }

    samtools {
        mem_per_thread = "5500M"
        cluster_time = 12.h
        cluster_cpus = 4
        cluster_memory = 14000.MB
    }

    flagstats {
        cluster_time = 6.h
        cluster_cpus = 1
        cluster_memory = 32000.MB
    }

    merge_abundance {
        cluster_time = 36.h
        cluster_cpus = 1
        cluster_memory = 32000.MB
        //TODO if large project 
        //cluster_memory = 122000.MB
    }

    metabat_abundance {
        num_threads = 2
        min_contig_length = 1000
        min_contig_depth = 2
        perc_id = 97
        cluster_time = 12.h
        //cluster_time = 48.h
        cluster_cpus = 2
        //cluster_memory = 64000.MB
        cluster_memory = 12000.MB
    }

    metabat2 {
        min_contig = 2000
        max_p = 95
        num_threads = 2
        //cluster_time = 12.h
        //cluster_cpus = 16
        //cluster_memory = 122000.MB
        cluster_time = 12.h
        cluster_cpus = 2
        cluster_memory = 12000.MB
    }

    maxbin2 {
        min_contig = 2000
        max_iteration = 50
        num_threads = 16
        prob_threshold = 0.9
        cluster_time = 12.h
        cluster_cpus = 16
        cluster_memory = 122000.MB
    }

    checkm {
        num_threads = 2
        cluster_time = 12.h
        cluster_cpus = 2
        cluster_memory = 48000.MB
        //cluster_time = 24.h
        //cluster_cpus = 4
        //cluster_memory = 122000.MB
    }

    merge {
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 24000.MB
    }

    keep_best_hit {
        evalue = 1e-05
        length = 50
        align_perc = 40
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 32000.MB
    }

    parse_bins {
        split="o__"
        cluster_time = 6.h
        cluster_cpus = 1
    }

    generate_gff {
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 96000.MB
    }

    bins_feature_table {
        cluster_time = 11.h
        cluster_cpus = 1
        cluster_memory = 64000.MB
    }

    beta_diversity {
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 64000.MB
    }

    trnascanse {
        num_threads = 4
        cluster_time = 12.h
        cluster_cpus = 4
        cluster_memory = 16000.MB
    }

    barrnap {
        num_threads = 16
        cluster_time = 12.h
        cluster_cpus = 16
        cluster_memory = 72000.MB
    }

    anvio_profile {
        num_threads = 12
        cluster_time = 12.h
        cluster_cpus = 12
        cluster_memory = 64000.MB
    }

    anvio_hmms {
        num_threads = 8
        cluster_time = 12.h
        cluster_cpus = 8
        cluster_memory = 48000.MB
    }

    bhtsne {
        num_threads = 1
        cluster_time = 24.h
        cluster_cpus = 1
        cluster_memory = 255000.MB
        perplexity = 30
        initial_dims = 50
        theta = 0.5
        no_dim = 2
    }

    kofamscan {
        num_threads = 4
        profiles = "$INSTALL_HOME/databases/kofam/2022-04-05/profiles"
        ko_list = "$INSTALL_HOME/databases/kofam/2022-04-05/ko_list"
        evalue = 1e-10
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 32000.MB
    }

    parse_kofam {
        num_threads = 1
        ref_database = "$INSTALL_HOME/databases/kofam/2022-04-05/kegg_ref_pathways_modules_combined.tsv"
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 64000.MB
    }

    cog_overrep {
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 99000.MB
    }

    kegg_overrep {
        cluster_time = 12.h
        cluster_cpus = 1
        cluster_memory = 99000.MB
    }

    DDA {
        do_deg_pairwise = "no"
        do_deg_glm = "no"
        fdr = 0.05
        pvalue = 0.05
        logfc = 1.5
        blocks = "no"
        treatments = "Treatment"
        mapping_files = "./mapping_file.tsv"
        cluster_time = 12.h
        cluster_cpus = 12
        cluster_memory = 64000.MB
    }

}

manifest {
    name            = "ShotgunMG_nextflow"
    author          = """Julien Tremblay"""
    homePage        = "https://github.com/ShotgunMG"
    description     = """Reads QC, co-assembly, abundance estimation, annotations and MAGs generation."""
    mainScript      = "shotgunmg.nf"
    nextflowVersion = "!>=22.10.1"
    version         = "1.4.0"
    doi             = "https://doi.org/10.1093/bib/bbac443"
}
